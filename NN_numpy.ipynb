{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow\n",
    "from keras.datasets.mnist import load_data\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        '''\n",
    "        Fit model on train set\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Model):\n",
    "    def __init__(self, model_dim: list) -> None: \n",
    "        '''\n",
    "        Args: input: input np ndarray of shape (1,N) \n",
    "            model_dim: NN dim [748,30,60,30,10]\n",
    "        '''\n",
    "        self.model_dim = model_dim\n",
    "        # w and b matrices excludes input layer:\n",
    "        self.weights = [np.random.randn(i,j) for i,j in zip(model_dim[:-1], model_dim[1:])] # normally distributed\n",
    "        self.biases = [np.random.randn(1,i) for i in model_dim[1:]] \n",
    "        self.lr_0 = 0.00001 # initial LR\n",
    "        self.batch_size = 10       \n",
    "        self.max_epochs = 1 \n",
    "        self.evaluate_every_n_epochs = 1\n",
    "        \n",
    "\n",
    "    def forwardpass(self, x, layer, activation_func, output_unit, mode = 'train') -> tuple[np.ndarray,np.ndarray] | np.ndarray:\n",
    "        '''\n",
    "        calculates the activations of one layer\n",
    "        Args: x: Activations from prev layer or input train/eval/test data \n",
    "        layer: layer number for which we are performing the pass\n",
    "            w: weights of current layer, indexing of 1st layer(input) starts from 0\n",
    "            b: baises of current layer, indexing of 1st layer(input) starts from 0\n",
    "        y = x'Tw + b\n",
    "        '''\n",
    "        w = self.weights[layer-1] # idx correction. 0 idx is weights b/w 1st and 2nd layer.\n",
    "        b = self.biases[layer-1]\n",
    "    \n",
    "        if layer == len(self.model_dim)-1: # if last layer, use output units instead\n",
    "            activation_func = output_unit\n",
    "        \n",
    "        z = np.dot(x,w) + b # and not W,x?\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return (z,activation_func(z)) #  .matmul or .dot ? \n",
    "\n",
    "        elif mode == 'eval':\n",
    "            return activation_func(z) #  .matmul or .dot ? \n",
    "        \n",
    "        else: return None\n",
    "\n",
    "    def costfunc(self, y, cost_func) -> None:\n",
    "        '''\n",
    "        (NOT USED)\n",
    "        Calculates values of objective function during evaluation only\n",
    "        MSE, RMSE, Cross Entropy etc.\n",
    "        Args: y : validation data labels\n",
    "            cost_func: objective function like MSE, RMSE, cross entropy\n",
    "        '''\n",
    "\n",
    "        # self.costs = cost_func(self.activations[-1],y)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backwardpass(self, x: np.ndarray, y: np.ndarray,\n",
    "                     optimzation_func, cost_func_deriv) -> None:\n",
    "        '''\n",
    "        Update params using gradient descent algo of all layers\n",
    "        GD, SGD, mini-batch SGD, AdaGrad, RMSProp, Adam etc.\n",
    "        Args: x : training data\n",
    "            y: training labels\n",
    "            backprop_func: function for \n",
    "        '''\n",
    "    \n",
    "        (self.weights, self.biases) = optimzation_func(self,self.lr_0, x,y, self.weights, self.biases,\n",
    "                                                     self.batch_size, cost_func_deriv)\n",
    "        \n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backprop(self, zs, activations, y, cost_func_deriv, activation_func_deriv, output_unit_deriv) -> tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "         Calculate gradients of pre-activations (x.W+b) for each layer\n",
    "         Args: zs : pre-activation outputs\n",
    "            activations : post-activation outputs\n",
    "            y : training labels\n",
    "        '''\n",
    "\n",
    "        w_gradient = [np.zeros_like(w_i) for w_i in self.weights] # i represents ith layer\n",
    "        b_gradient = [np.zeros_like(b_i) for b_i in self.biases]        \n",
    "\n",
    "        g = cost_func_deriv(activations[-1],y)\n",
    "\n",
    "        g = g * output_unit_deriv(zs[-1]) #(1,10)\n",
    "        w_gradient[-1] = np.dot(activations[-2].T,g) # Transpose and g.hT in OG eq   0,1,2 ; (30,1).(1,10) = (30,10)\n",
    "        b_gradient[-1] = g # (1,10)\n",
    "        g = np.dot(g, self.weights[-1].T) # (1,10).(10,30)=(1,30)\n",
    "\n",
    "        for layer in range(2,len(self.model_dim)):\n",
    "            g = g * activation_func_deriv(zs[-layer]) #(1,30)\n",
    "\n",
    "            w_gradient[-layer] = np.dot(activations[-(layer+1)].T,g) #(1,784).(1,30)=(784,30)\n",
    "            b_gradient[-layer] = g\n",
    "            g = np.dot(g, self.weights[-layer].T)\n",
    "\n",
    "            \n",
    "        return (w_gradient,b_gradient)\n",
    "\n",
    "    def evaluate(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        \n",
    "        predictions = []\n",
    "        for x_i in x:\n",
    "            activation = x_i\n",
    "            \n",
    "            for layer in range(1,len(self.model_dim)): # 0 idx is input\n",
    "                activation = self.forwardpass( activation, layer, sigmoid, sigmoid, mode= 'eval')\n",
    "            \n",
    "            predictions.append(np.argmax(activation))\n",
    "            \n",
    "        \n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        #calculate loss\n",
    "        val_loss = MSE(predictions,y)\n",
    "\n",
    "        print(f'Test/validation loss: {val_loss}')\n",
    "        print(predictions,y) #BUG: all predictions are same. perhaps check forwardpass for eval.\n",
    "        \n",
    "\n",
    "        \n",
    "        return sum(int(pred==y_i) for pred,y_i in zip(predictions,y))\n",
    "\n",
    "    def fit(self, train_dataloader: tuple[np.ndarray,np.ndarray], eval_dataloader: tuple[np.ndarray,np.ndarray],\n",
    "            lr_0: float, max_epochs: int = 1 , batch_size: int = 10, evaluate_every_n_epochs: int = 1) -> None:\n",
    "\n",
    "        train_x, train_y = train_dataloader\n",
    "        val_x, val_y = eval_dataloader\n",
    "        self.evaluate_every_n_epochs = evaluate_every_n_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr_0 = lr_0\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        if train_x.shape[1] != self.model_dim[0] or train_y.shape[1] != self.model_dim[-1]:\n",
    "            print(f\"training dataset not in correct dimension. \\\n",
    "                  Needed {self.model[0],self.model[-1]} \\\n",
    "                    received {train_x.shape[1], train_y.shape[1]}\")\n",
    "\n",
    "        for epoch in range(self.max_epochs): \n",
    "\n",
    "            self.backwardpass(train_x, train_y, minibatchSGD, MSE_derivative)\n",
    "            print(f'Epoch {epoch+1}/{self.max_epochs} done')\n",
    "            if epoch % evaluate_every_n_epochs == 0: \n",
    "                print(f'Evaluation: {self.evaluate(val_x, val_y)}/{val_y.shape[0]} correct')\n",
    "                \n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function to a layer of NN\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function derivative\n",
    "    '''\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def ReLu(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function to a layer of NN\n",
    "    '''\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function derivative \n",
    "    '''\n",
    "    # if x.all() > 0: #BUG: is .all correct??\n",
    "    #     return 1\n",
    "    # else: return 0\n",
    "    return (x>0)*1\n",
    "\n",
    "    \n",
    "\n",
    "def MSE(y: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    (NOT USED)\n",
    "    Cost function - Mean Squared Error\n",
    "    Args: y: predictions/ activations from output units\n",
    "        y_true: data labels \n",
    "    '''\n",
    "    n = y.shape[0]\n",
    "    cost = np.sum(np.absolute(y_true-y)**2)/(2.0*n)\n",
    "    return cost\n",
    "\n",
    "def MSE_derivative(y: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Args: y: predictions\n",
    "        y_true: data labels \n",
    "    '''\n",
    "    return y - y_true\n",
    "\n",
    "def BGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, w,b, cost_func_deriv) -> tuple[np.ndarray,np.ndarray]:\n",
    "    '''\n",
    "    Batched gradient decent algorithm\n",
    "    '''\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "    # x, y = zip(*trainset)\n",
    "\n",
    "    activations = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    zs = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    w_gradient = [np.zeros_like(w_i) for w_i in w] # i represents ith layer\n",
    "    b_gradient = [np.zeros_like(b_i) for b_i in b]        \n",
    "\n",
    "    # for each sample\n",
    "    for x, y in trainset:\n",
    "        activations[0] = x.reshape(1,-1)\n",
    "        zs[0] = x.reshape(1,-1)\n",
    "        \n",
    "        # forward pass\n",
    "        for layer in range(1,len(self.model_dim)): # 0 idx is input\n",
    "                zs[layer], activations[layer] = self.forwardpass( activations[layer-1], layer, sigmoid, sigmoid)\n",
    "\n",
    "        # backprop\n",
    "        delta_w_gradient, delta_b_gradient = self.backprop(zs,activations,y, cost_func_deriv, sigmoid_derivative, sigmoid_derivative)\n",
    "        \n",
    "        # accumulate gradients before applying\n",
    "        w_gradient = [wg_i + d_wg_i for wg_i, d_wg_i in zip(w_gradient,delta_w_gradient)]\n",
    "        b_gradient = [bg_i + d_bg_i for bg_i, d_bg_i in zip(b_gradient,delta_b_gradient)]\n",
    "\n",
    "    # apply gradients\n",
    "    w = [w_i - (epsilon_fixed/batch_size) * w_gradient_i for w_i, w_gradient_i in zip(w, w_gradient)]\n",
    "    b = [b_i - (epsilon_fixed/batch_size) * b_gradient_i for b_i, b_gradient_i in zip(b, b_gradient)]\n",
    "    \n",
    "    # TODO: evaluate_every_n_steps here\n",
    "\n",
    "    return (w, b)\n",
    "\n",
    "setattr(Network, 'BGD', BGD)\n",
    "\n",
    "\n",
    "def minibatchSGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, \n",
    "                 w, b, batch_size, cost_func_deriv ) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "    \n",
    "    mini_batches = [ trainset[i:i+batch_size] for i in range(0, len(trainset), batch_size)]\n",
    "    \n",
    "    for mini_batch in mini_batches:\n",
    "\n",
    "        mini_batch_x, mini_batch_y = zip(*mini_batch)\n",
    "        mini_batch_x = np.stack(mini_batch_x, axis = 0)\n",
    "        mini_batch_y = np.stack(mini_batch_y, axis = 0)\n",
    "        \n",
    "        (w, b) = BGD(self, epsilon_fixed, mini_batch_x, mini_batch_y, w, b, cost_func_deriv)\n",
    "    return (w, b)\n",
    "# setattr(Network, 'minibatchSGD', minibatchSGD)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist() -> tuple[tuple,tuple,tuple]:\n",
    "    '''\n",
    "    Args: None\n",
    "    Return: tuple(train_x,train_y), tuple(val_x,val_y), tuple(test_x,test_y) \n",
    "\n",
    "    train data -> 60k\n",
    "    test data -> 10k\n",
    "    image -> 28x28 = 728\n",
    "    train_x -> (50k,728) , train_y -> (50k,10)\n",
    "    val_x -> (10k,728), val_y -> (10k,10)\n",
    "    test_x -> (10k,728), test_y -> (10k,1)\n",
    "\n",
    "    '''\n",
    "    (trainX, trainy), (testX, testy) = load_data() #TODO: shuffle trainX\n",
    "\n",
    "    # train_data = [np.reshape(img,(784,1)) for img in trainX]  # 60k x (784,1), if imaginging NN arch horizontal\n",
    "    trainX = np.reshape(trainX,(-1,784))/255.0 # 60k x (1,784) = 60k x 784, if imagining NN arch vertical. ie row vector matrices\n",
    "    testX = np.reshape(testX,(-1,784))/255.0 # 10k x (1,784) = 10k x 784\n",
    "\n",
    "    train_data, train_label = trainX[:50000], trainy[:50000]\n",
    "    val_data, val_label = trainX[50000:], trainy[50000:]\n",
    "\n",
    "    train_label = np.array([vectorize_digit(label) for label in train_label])\n",
    "    \n",
    "    print(train_data.shape,train_label.shape)\n",
    "    print(val_data.shape,val_label.shape)\n",
    "    print(testX.shape,testy.shape)\n",
    "    return ((train_data,train_label),(val_data, val_label), (testX,testy) )\n",
    "\n",
    "\n",
    "def vectorize_digit(digit: np.uint) -> np.ndarray:\n",
    "    '''\n",
    "    Args: digit like 0-9\n",
    "    Return: vectorized form. 7 becomes [0,0,0,0,0,0,0,1,0,0]\n",
    "    '''\n",
    "    d = np.zeros(10)\n",
    "    d[digit] = 1.0\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "Epoch 1/20 done\n",
      "Test/validation loss: 5.0117\n",
      "[3 3 3 ... 3 3 3] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1017/10000 correct\n",
      "Epoch 2/20 done\n",
      "Test/validation loss: 10.47845\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1391/10000 correct\n",
      "Epoch 3/20 done\n",
      "Test/validation loss: 10.4884\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1391/10000 correct\n",
      "Epoch 4/20 done\n",
      "Test/validation loss: 10.5027\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1397/10000 correct\n",
      "Epoch 5/20 done\n",
      "Test/validation loss: 10.5108\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1398/10000 correct\n",
      "Epoch 6/20 done\n",
      "Test/validation loss: 10.5209\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1400/10000 correct\n",
      "Epoch 7/20 done\n",
      "Test/validation loss: 10.53305\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1402/10000 correct\n",
      "Epoch 8/20 done\n",
      "Test/validation loss: 10.51175\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1399/10000 correct\n",
      "Epoch 9/20 done\n",
      "Test/validation loss: 10.5126\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1406/10000 correct\n",
      "Epoch 10/20 done\n",
      "Test/validation loss: 10.5059\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1412/10000 correct\n",
      "Epoch 11/20 done\n",
      "Test/validation loss: 10.5232\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1422/10000 correct\n",
      "Epoch 12/20 done\n",
      "Test/validation loss: 10.52295\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1424/10000 correct\n",
      "Epoch 13/20 done\n",
      "Test/validation loss: 10.5361\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1411/10000 correct\n",
      "Epoch 14/20 done\n",
      "Test/validation loss: 10.52035\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1410/10000 correct\n",
      "Epoch 15/20 done\n",
      "Test/validation loss: 10.52285\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1411/10000 correct\n",
      "Epoch 16/20 done\n",
      "Test/validation loss: 10.52825\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1416/10000 correct\n",
      "Epoch 17/20 done\n",
      "Test/validation loss: 10.5436\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1414/10000 correct\n",
      "Epoch 18/20 done\n",
      "Test/validation loss: 10.52945\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1417/10000 correct\n",
      "Epoch 19/20 done\n",
      "Test/validation loss: 10.5142\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1419/10000 correct\n",
      "Epoch 20/20 done\n",
      "Test/validation loss: 10.5023\n",
      "[1 1 1 ... 1 1 0] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1416/10000 correct\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    (trainset,valset,testset) = load_mnist()\n",
    "    model = Network(model_dim=[trainset[0].shape[-1],30,trainset[1].shape[-1]])\n",
    "    model.fit(trainset, testset, lr_0 = 0.005,  max_epochs= 20, batch_size=10) #note validating on test set\n",
    "    \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Try making all vectors as column vectors, like done in book.\n",
    "* Print loss at each epoch for train set.\n",
    "* (DONE) Print loss at each epoch for val set.\n",
    "* Check weight/bias arrays are updating after each step.\n",
    "\n",
    "Observations:\n",
    "* NN doesnt necessarily has same predictions for consequtive epochs. \n",
    "    * activation of output unit is all 0s except one(which is also -6 to -100 decimal place). why are figures absolutely 0? for Sigmoid func, this means most of the pred are around the centre.\n",
    "* delta_w_gradient from backprop for 784x30 nodes are 0.00, hence first layer is not learning, but 2nd is.\n",
    "    * possibly the loop on layers in incorrect. YES.\n",
    "    * shapes are incorrect during backprop. YES.\n",
    "    * Not all are 0. it is learning, but has most of them 0.00.\n",
    "\n",
    "* Hyperparameter tuning:\n",
    "    * lr_0 = 0.005, batch_size = 10, epochs=5: shows different predictions and improves.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=5: shows same predictions but improves faster.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=20: 2645/10k.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=20: 1416/10k.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "from_scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
