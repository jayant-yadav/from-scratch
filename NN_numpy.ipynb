{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow\n",
    "from keras.datasets.mnist import load_data\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        '''\n",
    "        Fit model on train set\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Model):\n",
    "    def __init__(self, model_dim: list, batch_size) -> None: \n",
    "        '''\n",
    "        Args: input: input np ndarray of shape (1,N) \n",
    "            model_dim: NN dim [748,30,60,30,10]\n",
    "        '''\n",
    "        self.model_dim = model_dim\n",
    "        # w and b matrices excludes input layer:\n",
    "        self.weights = [np.random.randn(i,j) for i,j in zip(model_dim[:-1], model_dim[1:])] # normally distributed\n",
    "        self.biases = [np.random.randn(1,i) for i in model_dim[1:]] \n",
    "        self.lr_0 = 0.00001 # initial LR\n",
    "        self.batch_size = batch_size        \n",
    "        self.epochs = 1 \n",
    "        \n",
    "\n",
    "    def forwardpass(self, x, layer, activation_func, output_unit, mode = 'train') -> tuple[np.ndarray,np.ndarray]:\n",
    "        '''\n",
    "        calculates the activations of one layer\n",
    "        Args: x: Activations from prev layer or input train/eval/test data \n",
    "        layer: layer number for which we are performing the pass\n",
    "            w: weights of current layer, indexing of 1st layer(input) starts from 0\n",
    "            b: baises of current layer, indexing of 1st layer(input) starts from 0\n",
    "        y = x'Tw + b\n",
    "        '''\n",
    "        w = self.weights[layer-1] # idx correction. 0 idx is weights b/w 1st and 2nd layer.\n",
    "        b = self.biases[layer-1]\n",
    "        assert type(x) == np.ndarray, f\"needed nd.array for activations, received : {type(x)}\"\n",
    "\n",
    "        if mode == 'train':\n",
    "\n",
    "            if layer == len(self.model_dim)-1: # if last layer, use output units instead\n",
    "\n",
    "                activation_func = output_unit\n",
    "                z = np.dot(x,w) + b # and not W,x?\n",
    "\n",
    "            return (z,activation_func(z)) #  .matmul or .dot ? \n",
    "\n",
    "        else: return None\n",
    "\n",
    "    def costfunc(self, y, cost_func) -> None:\n",
    "        '''\n",
    "        (NOT USED)\n",
    "        Calculates values of objective function during evaluation only\n",
    "        MSE, RMSE, Cross Entropy etc.\n",
    "        Args: y : validation data labels\n",
    "            cost_func: objective function like MSE, RMSE, cross entropy\n",
    "        '''\n",
    "\n",
    "        self.costs = cost_func(self.activations[-1],y)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backwardpass(self, x: np.ndarray, y, optimzation_func, cost_func_deriv) -> None:\n",
    "        '''\n",
    "        Update params using gradient descent algo of all layers\n",
    "        GD, SGD, mini-batch SGD, AdaGrad, RMSProp, Adam etc.\n",
    "        Args: x : training data\n",
    "            y: training labels\n",
    "            backprop_func: function for \n",
    "        '''\n",
    "        (self.weights, self.biases) = optimzation_func(self,self.lr_0, x,y, self.weights, self.biases,\n",
    "                                                     self.batch_size, cost_func_deriv)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backprop(self, zs, activations, y, cost_func_deriv, activation_func_deriv, output_unit_deriv) -> tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "         Calculate gradients of pre-activations (x.W+b) for each layer\n",
    "         Args: zs : pre-activation outputs\n",
    "            activations : post-activation outputs\n",
    "            y : training labels\n",
    "        '''\n",
    "\n",
    "        w_gradient = [np.zeros_like(w_i) for w_i in self.weights] # i represents ith layer\n",
    "        b_gradient = [np.zeros_like(b_i) for b_i in self.biases]        \n",
    "\n",
    "        # graident at cost function\n",
    "        g = cost_func_deriv(activations[-1],y)\n",
    "\n",
    "        # graident at output unit\n",
    "        g = g * output_unit_deriv(zs[-1])\n",
    "        w_gradient[-1] = np.dot(g, activations[-2].T)\n",
    "        b_gradient[-1] = g\n",
    "        g = np.dot(self.weights[-1].T, g)\n",
    "\n",
    "        # gradients at hidden units\n",
    "        for layer in range(len(self.model_dim)-1, 0, -1):\n",
    "            print(layer)\n",
    "            g = g * activation_func_deriv(zs[layer])\n",
    "            w_gradient[layer] = np.dot(g, activations[layer-1].T) # BUG:out of bound\n",
    "            b_gradient[layer] = g\n",
    "            g = np.dot(self.weights[layer].T, g)\n",
    "\n",
    "        return (w_gradient,b_gradient)\n",
    "\n",
    "    def evaluate(self) -> None:\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        # batch_size = y\n",
    "        # for layer in range(1,self.model_dim):\n",
    "        #     self.forwardpass(layer, ReLu)\n",
    "\n",
    "        # y = self.forwardpass(self.model_dim[-1], sigmoid)\n",
    "        # loss = self.costfunc(y)\n",
    "        # print(f\"Evaluation loss: {loss}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self, trainset: tuple[np.ndarray,np.ndarray]) -> None:\n",
    "\n",
    "        train_x, train_y = trainset\n",
    "        # val_x, val_y = trainset[1]\n",
    "    \n",
    "        if train_x.shape[1] != self.model_dim[0] or train_y.shape[1] != self.model_dim[-1]:\n",
    "            print(f\"training dataset not in correct dimension. \\\n",
    "                  Needed {self.model[0],self.model[-1]} \\\n",
    "                    received {train_x.shape[1], train_y.shape[1]}\")\n",
    "\n",
    "        for epoch in range(self.epochs): #TODO: send to self.fit\n",
    "\n",
    "            self.backwardpass(train_x, train_y, minibatchSGD, MSE_derivative)\n",
    "            print('Epoch {epoch}/{epochs} done')\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function to a layer of NN\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function derivative\n",
    "    '''\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def ReLu(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function to a layer of NN\n",
    "    '''\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function derivative \n",
    "    '''\n",
    "    if x > 0: \n",
    "        return 1\n",
    "\n",
    "    else: return 0\n",
    "\n",
    "def MSE(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    (NOT USED)\n",
    "    Cost function - Mean Squared Error\n",
    "    Args: x: activations from output units\n",
    "        y: data labels \n",
    "    '''\n",
    "    n = x.shape[-1]\n",
    "    cost = np.sum(np.absolute(y-x)**2)/(2.0*n)\n",
    "    return cost\n",
    "\n",
    "def MSE_derivative(y: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    return y - y_true\n",
    "\n",
    "def BGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, w,b, cost_func_deriv) -> tuple[np.ndarray,np.ndarray]:\n",
    "    '''\n",
    "    Batched gradient decent algorithm\n",
    "    '''\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "    # x, y = zip(*trainset)\n",
    "\n",
    "    activations = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    zs = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    w_gradient = [np.zeros_like(w_i) for w_i in self.weights] # i represents ith layer\n",
    "    b_gradient = [np.zeros_like(b_i) for b_i in self.biases]        \n",
    "\n",
    "    # for each sample\n",
    "    for x, y in trainset:\n",
    "        activations[0] = x\n",
    "        zs[0] = x\n",
    "\n",
    "        # forward pass\n",
    "        for layer in range(1,len(self.model_dim)): # 0 idx is input\n",
    "                zs[layer], activations[layer] = self.forwardpass( activations[layer-1], layer, ReLu, sigmoid)\n",
    "\n",
    "        # backprop\n",
    "        delta_w_gradient, delta_b_gradient = self.backprop(zs,activations,y, cost_func_deriv, ReLu_derivative, sigmoid_derivative)\n",
    "\n",
    "        # accumulate gradients before applying\n",
    "        w_gradient = [wg_i + d_wg_i for wg_i, d_wg_i in zip(w_gradient,delta_w_gradient)]\n",
    "        b_gradient = [bg_i + d_bg_i for bg_i, d_bg_i in zip(b_gradient,delta_b_gradient)]\n",
    "\n",
    "    # apply gradients\n",
    "    w = [w_i - (epsilon_fixed/batch_size) * w_gradient_i for w_i, w_gradient_i in zip(w, w_gradient)]\n",
    "    b = [b_i - (epsilon_fixed/batch_size) * b_gradient_i for b_i, b_gradient_i in zip(b, b_gradient)]\n",
    "    \n",
    "    return (w, b)\n",
    "\n",
    "setattr(Network, 'BGD', BGD)\n",
    "\n",
    "\n",
    "def minibatchSGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, \n",
    "                 w, b, batch_size, cost_func_deriv ) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "    \n",
    "    mini_batches = [ trainset[i:i+batch_size] for i in range(0, len(trainset), batch_size)]\n",
    "    \n",
    "    for mini_batch in mini_batches:\n",
    "\n",
    "        mini_batch_x, mini_batch_y = zip(*mini_batch)\n",
    "        mini_batch_x = np.stack(mini_batch_x, axis = 0)\n",
    "        mini_batch_y = np.stack(mini_batch_y, axis = 0)\n",
    "        \n",
    "        (w, b) = BGD(self, epsilon_fixed, mini_batch_x, mini_batch_y, w, b, cost_func_deriv)\n",
    "\n",
    "    return (w, b)\n",
    "# setattr(Network, 'minibatchSGD', minibatchSGD)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist() -> tuple[tuple,tuple,tuple]:\n",
    "    '''\n",
    "    Args: None\n",
    "    Return: tuple(train_x,train_y), tuple(val_x,val_y), tuple(test_x,test_y) \n",
    "\n",
    "    train data -> 60k\n",
    "    test data -> 10k\n",
    "    image -> 28x28 = 728\n",
    "    train_x -> (50k,728) , train_y -> (50k,10)\n",
    "    val_x -> (10k,728), val_y -> (10k,10)\n",
    "    test_x -> (10k,728), test_y -> (10k,1)\n",
    "\n",
    "    '''\n",
    "    (trainX, trainy), (testX, testy) = load_data() #TODO: shuffle trainX\n",
    "\n",
    "    # train_data = [np.reshape(img,(784,1)) for img in trainX]  # 60k x (784,1), if imaginging NN arch horizontal\n",
    "    trainX = np.reshape(trainX,(-1,784)) # 60k x (1,784) = 60k x 784, if imagining NN arch vertical\n",
    "    testX = np.reshape(testX,(-1,784)) # 10k x (1,784) = 10k x 784\n",
    "\n",
    "    train_data, train_label = trainX[:50000], trainy[:50000]\n",
    "    val_data, val_label = trainX[50000:], trainy[50000:]\n",
    "\n",
    "    train_label = np.array([vectorize_digit(label) for label in train_label])\n",
    "    val_label = np.array([vectorize_digit(label) for label in val_label])\n",
    "    \n",
    "    print(train_data.shape,train_label.shape)\n",
    "    print(val_data.shape,val_label.shape)\n",
    "    print(testX.shape,testy.shape)\n",
    "    #TODO: Normalize data\n",
    "    return ((train_data,train_label),(val_data, val_label), (testX,testy) )\n",
    "\n",
    "\n",
    "def vectorize_digit(digit: np.uint) -> np.ndarray:\n",
    "    '''\n",
    "    Args: digit like 0-9\n",
    "    Return: vectorized form. 7 becomes [0,0,0,0,0,0,0,1,0,0]\n",
    "    '''\n",
    "    d = np.zeros(10)\n",
    "    d[digit] = 1.0\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(10000, 784) (10000,)\n",
      "1 2\n",
      "2 2\n",
      "here\n",
      "(10, 10) [[  -779.97008356  12195.90326027   2199.07734558   4208.75031492\n",
      "   18054.38515053  -8539.06514356   -775.87945746  -9112.79074964\n",
      "    1416.26004049 -15300.435469  ]\n",
      " [  3854.56028535  21420.60004598  -1324.94630852   8194.42872179\n",
      "   19313.55495826  -9348.3893125   -4065.06877677 -18427.07686543\n",
      "   -3737.66822006 -10253.26960944]\n",
      " [ -1104.96401489  19564.09389513 -16624.71040869   6921.05548406\n",
      "   13669.31664024 -10356.53610554  -3368.45451496 -18560.8224169\n",
      "   -7039.14691342 -21014.36168775]\n",
      " [  3136.34701772  20481.47308119   3999.03195      2783.56181963\n",
      "   10818.41814131  -9972.97640726  -5153.75160521  -1323.08661542\n",
      "   -5388.64915501 -12408.94074437]\n",
      " [  5980.93373185   6078.18283476  -1518.4319762    3392.49534647\n",
      "     722.85558129  -3554.38130273   2198.88375533  -6531.9126437\n",
      "    2584.22766078  -9779.96736223]\n",
      " [  3466.7971194   21457.51752357   6625.28121018   -146.22665761\n",
      "   14168.43846766  -9682.60284839   3898.26707947 -15477.83805236\n",
      "  -12489.61465714  -7205.21614963]\n",
      " [  7507.7055544   15882.93134309 -13152.68272243  -3117.61904954\n",
      "    8110.92150842   1530.66919664   -369.02075135  -6754.80774222\n",
      "   -2311.05343556 -18248.0373247 ]\n",
      " [  2786.05285564  30578.9966325   -6543.84102465   1033.12252118\n",
      "   24072.08461845  -6113.1915255    2203.69009187 -14986.31361351\n",
      "   -7890.92082439 -13756.86030344]\n",
      " [ 14887.30601655   8609.48724894  10852.33441481   4708.80878479\n",
      "   12217.81906397  -3860.48740727   2372.42342982  -7291.8608515\n",
      "      50.82776122  -8246.07095414]\n",
      " [  7227.67491512  19576.22278374  -3686.31259671   3769.23173951\n",
      "   11917.19041742 -11090.24930274   3971.28022581 -18746.05854441\n",
      "   -3342.64940845 -16697.11396457]]\n",
      "[[0.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000 3.12298209e-064\n",
      "  1.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  1.00000000e+000 1.00000000e+000 5.44909000e-161 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      "  1.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_22796/459828041.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  print(1.0 / (1.0 + np.exp(-x)))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_22796/459828041.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    (trainset,valset,testset) = load_mnist()\n",
    "    model = Network(model_dim=[trainset[0].shape[-1],30,trainset[1].shape[-1]],\n",
    "                    batch_size=10)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "from_scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
