{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow\n",
    "from keras.datasets.mnist import load_data\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        '''\n",
    "        Fit model on train set\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Model):\n",
    "    def __init__(self, model_dim: list) -> None: \n",
    "        '''\n",
    "        Args: input: input np ndarray of shape (1,N) \n",
    "            model_dim: NN dim [748,30,60,30,10]\n",
    "        '''\n",
    "        self.model_dim = model_dim\n",
    "        # w and b matrices excludes input layer:\n",
    "        self.weights = [np.random.randn(i,j) for i,j in zip(model_dim[:-1], model_dim[1:])] # normally distributed\n",
    "        self.biases = [np.random.randn(1,i) for i in model_dim[1:]] \n",
    "        self.lr_0 = 0.00001 # initial LR\n",
    "        self.batch_size = 10       \n",
    "        self.max_epochs = 1 \n",
    "        self.evaluate_every_n_epochs = 1\n",
    "        \n",
    "\n",
    "    def forwardpass(self, x, layer, activation_func, output_unit, mode = 'train') -> tuple[np.ndarray,np.ndarray] | np.ndarray:\n",
    "        '''\n",
    "        calculates the activations of one layer\n",
    "        Args: x: Activations from prev layer or input train/eval/test data \n",
    "        layer: layer number for which we are performing the pass\n",
    "            w: weights of current layer, indexing of 1st layer(input) starts from 0\n",
    "            b: baises of current layer, indexing of 1st layer(input) starts from 0\n",
    "        y = x'Tw + b\n",
    "        '''\n",
    "        w = self.weights[layer-1] # idx correction. 0 idx is weights b/w 1st and 2nd layer.\n",
    "        b = self.biases[layer-1]\n",
    "    \n",
    "        if layer == len(self.model_dim)-1: # if last layer, use output units instead\n",
    "            activation_func = output_unit\n",
    "        \n",
    "        z = np.dot(x,w) + b # and not W,x?\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return (z,activation_func(z)) \n",
    "\n",
    "        elif mode == 'eval':\n",
    "            return activation_func(z) \n",
    "        \n",
    "        else: return None\n",
    "\n",
    "    def costfunc(self, y, cost_func) -> None:\n",
    "        '''\n",
    "        (NOT USED)\n",
    "        Calculates values of objective function during evaluation only\n",
    "        MSE, RMSE, Cross Entropy etc.\n",
    "        Args: y : validation data labels\n",
    "            cost_func: objective function like MSE, RMSE, cross entropy\n",
    "        '''\n",
    "\n",
    "        # self.costs = cost_func(self.activations[-1],y)\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backwardpass(self, x: np.ndarray, y: np.ndarray,\n",
    "                     optimzation_func, cost_func_deriv) -> None:\n",
    "        '''\n",
    "        Update params using gradient descent algo of all layers\n",
    "        GD, SGD, mini-batch SGD, AdaGrad, RMSProp, Adam etc.\n",
    "        Args: x : training data\n",
    "            y: training labels\n",
    "            backprop_func: function for \n",
    "        '''\n",
    "    \n",
    "        (self.weights, self.biases) = optimzation_func(self,self.lr_0, x,y, self.weights, self.biases,\n",
    "                                                     self.batch_size, cost_func_deriv)\n",
    "        \n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def backprop(self, zs, activations, y, cost_func_deriv, activation_func_deriv, output_unit_deriv) -> tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "         Calculate gradients of pre-activations (x.W+b) for each layer\n",
    "         Args: zs : pre-activation outputs\n",
    "            activations : post-activation outputs\n",
    "            y : training labels\n",
    "        '''\n",
    "\n",
    "        w_gradient = [np.zeros_like(w_i) for w_i in self.weights] # i represents ith layer\n",
    "        b_gradient = [np.zeros_like(b_i) for b_i in self.biases]        \n",
    "\n",
    "        g = cost_func_deriv(activations[-1],y)\n",
    "\n",
    "        g = g * output_unit_deriv(zs[-1]) #(1,10)\n",
    "        w_gradient[-1] = np.dot(activations[-2].T,g) # Transpose and g.hT in OG eq   0,1,2 ; (30,1).(1,10) = (30,10)\n",
    "        b_gradient[-1] = g # (1,10)\n",
    "        g = np.dot(g, self.weights[-1].T) # (1,10).(10,30)=(1,30)\n",
    "\n",
    "        for layer in range(2,len(self.model_dim)):\n",
    "            g = g * activation_func_deriv(zs[-layer]) #(1,30)\n",
    "\n",
    "            w_gradient[-layer] = np.dot(activations[-(layer+1)].T,g) #(1,784).(1,30)=(784,30)\n",
    "            b_gradient[-layer] = g\n",
    "            g = np.dot(g, self.weights[-layer].T)\n",
    "\n",
    "            \n",
    "        return (w_gradient,b_gradient)\n",
    "\n",
    "    def evaluate(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        '''\n",
    "        Evaluate model on validation or test set\n",
    "        '''\n",
    "        \n",
    "        predictions = []\n",
    "        for x_i in x:\n",
    "            activation = x_i\n",
    "            \n",
    "            for layer in range(1,len(self.model_dim)): # 0 idx is input\n",
    "                activation = self.forwardpass( activation, layer, sigmoid, sigmoid, mode= 'eval')\n",
    "            \n",
    "            predictions.append(np.argmax(activation))\n",
    "            \n",
    "        \n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        #calculate loss\n",
    "        val_loss = MSE(predictions,y)\n",
    "\n",
    "        print(f'Test/validation loss: {val_loss}')\n",
    "        print(predictions,y) #BUG: all predictions are same. perhaps check forwardpass for eval.\n",
    "         \n",
    "        return sum(int(pred==y_i) for pred,y_i in zip(predictions,y))\n",
    "\n",
    "    def fit(self, train_dataloader: tuple[np.ndarray,np.ndarray], eval_dataloader: tuple[np.ndarray,np.ndarray],\n",
    "            lr_0: float, max_epochs: int = 1 , batch_size: int = 10, evaluate_every_n_epochs: int = 1) -> None:\n",
    "\n",
    "        train_x, train_y = train_dataloader\n",
    "        val_x, val_y = eval_dataloader\n",
    "        self.evaluate_every_n_epochs = evaluate_every_n_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr_0 = lr_0\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        if train_x.shape[1] != self.model_dim[0] or train_y.shape[1] != self.model_dim[-1]:\n",
    "            print(f\"training dataset not in correct dimension. \\\n",
    "                  Needed {self.model_dim[0],self.model_dim[-1]} \\\n",
    "                    received {train_x.shape[1], train_y.shape[1]}\")\n",
    "\n",
    "        for epoch in range(self.max_epochs): \n",
    "\n",
    "            self.backwardpass(train_x, train_y, minibatchSGD, MSE_derivative)\n",
    "            print(f'Epoch {epoch+1}/{self.max_epochs} done')\n",
    "            \n",
    "            if epoch % evaluate_every_n_epochs == 0: \n",
    "                print(f'Evaluation: {self.evaluate(val_x, val_y)}/{val_y.shape[0]} correct')\n",
    "                \n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function to a layer of NN\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Sigmoid function derivative\n",
    "    '''\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def ReLu(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function to a layer of NN\n",
    "    '''\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Relu function derivative \n",
    "    '''\n",
    "    # if x.all() > 0: #BUG: is .all correct??\n",
    "    #     return 1\n",
    "    # else: return 0\n",
    "    return (x>0)*1\n",
    "\n",
    "    \n",
    "\n",
    "def MSE(y: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    (NOT USED)\n",
    "    Cost function - Mean Squared Error\n",
    "    Args: y: predictions/ activations from output units\n",
    "        y_true: data labels \n",
    "    '''\n",
    "    n = y.shape[0]\n",
    "    cost = np.sum(np.absolute(y-y_true)**2)/(2.0*n)\n",
    "    return cost\n",
    "\n",
    "def MSE_derivative(y: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Args: y: predictions\n",
    "        y_true: data labels \n",
    "    '''\n",
    "    return y - y_true\n",
    "\n",
    "def BGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, w,b, cost_func_deriv) -> tuple[np.ndarray,np.ndarray]:\n",
    "    '''\n",
    "    Batched gradient decent algorithm\n",
    "    '''\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "\n",
    "\n",
    "    activations = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    zs = [np.zeros((1,i)) for i in self.model_dim] # including input and output of the network\n",
    "    w_gradient = [np.zeros_like(w_i) for w_i in w] # i represents ith layer\n",
    "    b_gradient = [np.zeros_like(b_i) for b_i in b]        \n",
    "\n",
    "    # for each sample\n",
    "    for x, y in trainset:\n",
    "        activations[0] = x.reshape(1,-1)\n",
    "        zs[0] = x.reshape(1,-1)\n",
    "        \n",
    "        # forward pass\n",
    "        for layer in range(1,len(self.model_dim)): # 0 idx is input\n",
    "                zs[layer], activations[layer] = self.forwardpass( activations[layer-1], layer, sigmoid, sigmoid)\n",
    "\n",
    "        # backprop\n",
    "        delta_w_gradient, delta_b_gradient = self.backprop(zs,activations,y, cost_func_deriv, sigmoid_derivative, sigmoid_derivative)\n",
    "        \n",
    "        # accumulate gradients before applying\n",
    "        w_gradient = [wg_i + d_wg_i for wg_i, d_wg_i in zip(w_gradient,delta_w_gradient)]\n",
    "        b_gradient = [bg_i + d_bg_i for bg_i, d_bg_i in zip(b_gradient,delta_b_gradient)]\n",
    "\n",
    "    # apply gradients\n",
    "    w = [w_i - (epsilon_fixed/batch_size) * w_gradient_i for w_i, w_gradient_i in zip(w, w_gradient)]\n",
    "    b = [b_i - (epsilon_fixed/batch_size) * b_gradient_i for b_i, b_gradient_i in zip(b, b_gradient)]\n",
    "    \n",
    "    # TODO: evaluate_every_n_steps here\n",
    "\n",
    "    return (w, b)\n",
    "\n",
    "setattr(Network, 'BGD', BGD)\n",
    "\n",
    "\n",
    "def minibatchSGD(self, epsilon_fixed, x: np.ndarray, y: np.ndarray, \n",
    "                 w, b, batch_size, cost_func_deriv ) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "\n",
    "\n",
    "    trainset = list(zip(x,y))\n",
    "    np.random.shuffle(trainset) #shuffle in-place\n",
    "    \n",
    "    mini_batches = [ trainset[i:i+batch_size] for i in range(0, len(trainset), batch_size)]\n",
    "    \n",
    "    for mini_batch in mini_batches:\n",
    "\n",
    "        mini_batch_x, mini_batch_y = zip(*mini_batch)\n",
    "        mini_batch_x = np.stack(mini_batch_x, axis = 0)\n",
    "        mini_batch_y = np.stack(mini_batch_y, axis = 0)\n",
    "        \n",
    "        (w, b) = BGD(self, epsilon_fixed, mini_batch_x, mini_batch_y, w, b, cost_func_deriv)\n",
    "    return (w, b)\n",
    "# setattr(Network, 'minibatchSGD', minibatchSGD)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist() -> tuple[tuple,tuple,tuple]:\n",
    "    '''\n",
    "    Args: None\n",
    "    Return: tuple(train_x,train_y), tuple(val_x,val_y), tuple(test_x,test_y) \n",
    "\n",
    "    train data -> 60k\n",
    "    test data -> 10k\n",
    "    image -> 28x28 = 728\n",
    "    train_x -> (50k,728) , train_y -> (50k,10)\n",
    "    val_x -> (10k,728), val_y -> (10k,10)\n",
    "    test_x -> (10k,728), test_y -> (10k,1)\n",
    "\n",
    "    '''\n",
    "    (trainX, trainy), (testX, testy) = load_data() #TODO: shuffle trainX\n",
    "\n",
    "    # train_data = [np.reshape(img,(784,1)) for img in trainX]  # 60k x (784,1), if imaginging NN arch horizontal\n",
    "    trainX = np.reshape(trainX,(-1,784))/255.0 # 60k x (1,784) = 60k x 784, if imagining NN arch vertical. ie row vector matrices\n",
    "    testX = np.reshape(testX,(-1,784))/255.0 # 10k x (1,784) = 10k x 784\n",
    "\n",
    "    train_data, train_label = trainX[:50000], trainy[:50000]\n",
    "    val_data, val_label = trainX[50000:], trainy[50000:]\n",
    "\n",
    "    train_label = np.array([vectorize_digit(label) for label in train_label])\n",
    "    # train_label = train_label.reshape(50000,10)\n",
    "    # val_label = val_label.reshape(10000,1)\n",
    "    # testy = testy.reshape(10000,1)\n",
    "    \n",
    "    print(train_data.shape,train_label.shape)\n",
    "    print(val_data.shape,val_label.shape)\n",
    "    print(testX.shape,testy.shape)\n",
    "    return ((train_data,train_label),(val_data, val_label), (testX,testy) )\n",
    "\n",
    "\n",
    "def vectorize_digit(digit: np.uint) -> np.ndarray:\n",
    "    '''\n",
    "    Args: digit like 0-9\n",
    "    Return: vectorized form. 7 becomes [0,0,0,0,0,0,0,1,0,0]\n",
    "    '''\n",
    "    d = np.zeros(10)\n",
    "    d[digit] = 1.0\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "Epoch 1/30 done\n",
      "Test/validation loss: 5.7433\n",
      "[6 6 6 ... 4 4 6] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 998/10000 correct\n",
      "Epoch 2/30 done\n",
      "Test/validation loss: 8.5998\n",
      "[7 6 7 ... 0 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1149/10000 correct\n",
      "Epoch 3/30 done\n",
      "Test/validation loss: 8.2284\n",
      "[7 6 7 ... 0 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1250/10000 correct\n",
      "Epoch 4/30 done\n",
      "Test/validation loss: 8.05395\n",
      "[7 6 7 ... 0 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1321/10000 correct\n",
      "Epoch 5/30 done\n",
      "Test/validation loss: 7.9247\n",
      "[7 6 7 ... 0 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1372/10000 correct\n",
      "Epoch 6/30 done\n",
      "Test/validation loss: 7.86455\n",
      "[7 6 7 ... 0 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1400/10000 correct\n",
      "Epoch 7/30 done\n",
      "Test/validation loss: 7.78925\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1426/10000 correct\n",
      "Epoch 8/30 done\n",
      "Test/validation loss: 7.7192\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1459/10000 correct\n",
      "Epoch 9/30 done\n",
      "Test/validation loss: 7.66665\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1502/10000 correct\n",
      "Epoch 10/30 done\n",
      "Test/validation loss: 7.6078\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1513/10000 correct\n",
      "Epoch 11/30 done\n",
      "Test/validation loss: 7.6007\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1534/10000 correct\n",
      "Epoch 12/30 done\n",
      "Test/validation loss: 7.58525\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1557/10000 correct\n",
      "Epoch 13/30 done\n",
      "Test/validation loss: 7.55655\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1578/10000 correct\n",
      "Epoch 14/30 done\n",
      "Test/validation loss: 7.55075\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1603/10000 correct\n",
      "Epoch 15/30 done\n",
      "Test/validation loss: 7.514\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1621/10000 correct\n",
      "Epoch 16/30 done\n",
      "Test/validation loss: 7.50305\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1637/10000 correct\n",
      "Epoch 17/30 done\n",
      "Test/validation loss: 7.4862\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1654/10000 correct\n",
      "Epoch 18/30 done\n",
      "Test/validation loss: 7.4774\n",
      "[7 6 7 ... 5 8 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1678/10000 correct\n",
      "Epoch 19/30 done\n",
      "Test/validation loss: 7.46805\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1685/10000 correct\n",
      "Epoch 20/30 done\n",
      "Test/validation loss: 7.46955\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1707/10000 correct\n",
      "Epoch 21/30 done\n",
      "Test/validation loss: 7.4575\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1726/10000 correct\n",
      "Epoch 22/30 done\n",
      "Test/validation loss: 7.4519\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1740/10000 correct\n",
      "Epoch 23/30 done\n",
      "Test/validation loss: 7.4544\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1752/10000 correct\n",
      "Epoch 24/30 done\n",
      "Test/validation loss: 7.467\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1775/10000 correct\n",
      "Epoch 25/30 done\n",
      "Test/validation loss: 7.47245\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1788/10000 correct\n",
      "Epoch 26/30 done\n",
      "Test/validation loss: 7.4811\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1808/10000 correct\n",
      "Epoch 27/30 done\n",
      "Test/validation loss: 7.50205\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1820/10000 correct\n",
      "Epoch 28/30 done\n",
      "Test/validation loss: 7.50485\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1840/10000 correct\n",
      "Epoch 29/30 done\n",
      "Test/validation loss: 7.51595\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1852/10000 correct\n",
      "Epoch 30/30 done\n",
      "Test/validation loss: 7.5151\n",
      "[7 6 7 ... 5 7 9] [7 2 1 ... 4 5 6]\n",
      "Evaluation: 1872/10000 correct\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    (trainset,valset,testset) = load_mnist()\n",
    "    model = Network(model_dim=[trainset[0].shape[-1],30,trainset[1].shape[-1]])\n",
    "    model.fit(trainset, testset, lr_0 = 0.002,  max_epochs= 30, batch_size=16) #note validating on test set\n",
    "    \n",
    "  #TODO: try trainset as validation set.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Try making all vectors as column vectors, like done in book.\n",
    "* Print loss at each epoch for train set.\n",
    "* (DONE) Print loss at each epoch for val set.\n",
    "* Check weight/bias arrays are updating after each step.\n",
    "\n",
    "Observations:\n",
    "* NN doesnt necessarily has same predictions for consequtive epochs. \n",
    "    * activation of output unit is all 0s except one(which is also -6 to -100 decimal place). why are figures absolutely 0? for Sigmoid func, this means most of the pred are around the centre.\n",
    "* delta_w_gradient from backprop for 784x30 nodes are 0.00, hence first layer is not learning, but 2nd is.\n",
    "    * possibly the loop on layers in incorrect. YES.\n",
    "    * shapes are incorrect during backprop. YES.\n",
    "    * Not all are 0. it is learning, but has most of them 0.00.\n",
    "* loss and correct predictions have no correlation. sometimes, higher loss for more correct predictions sometimes low.\n",
    "\n",
    "* Hyperparameter tuning:\n",
    "    * lr_0 = 0.005, batch_size = 10, epochs=5: shows different predictions and improves.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=5: shows same predictions but improves faster.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=20: 2645/10k.\n",
    "    * lr_0 = 0.0005 with batch_size = 10, epochs=50: 1673/10k.\n",
    "    * lr_0 = 0.005 with batch_size = 10, epochs=20: 1416/10k.\n",
    "    * lr_0 = 0.005 with batch_size = 10, epochs=50: 2173/10k.\n",
    "    * lr_0 = 0.001 with batch_size = 10, epochs=30: 2665/10k.\n",
    "    * lr_0 = 0.0015 with batch_size = 10, epochs=30: 2756/10k.\n",
    "    * lr_0 = 0.002 with batch_size = 10, epochs=30: 4168/10k.\n",
    "    * lr_0 = 0.0025 with batch_size = 10, epochs=30: 2770/10k.\n",
    "    * lr_0 = 0.003 with batch_size = 10, epochs=30: 1739/10k.\n",
    "    * lr_0 = 0.002 with batch_size = 10, epochs=100: 6755/10k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 µs ± 36.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# b_1 = np.random.randn(1,30)\n",
    "# w_1 = np.random.randn(784,30)\n",
    "# x_1 = np.random.randn(1,784)\n",
    "# z_1 = np.dot(x_1,w_1)+b_1\n",
    "# derv_1 = sigmoid_derivative(z_1)\n",
    "# print(z_1.shape, z_1)\n",
    "\n",
    "\n",
    "b_2 = np.random.randn(30,1)\n",
    "w_2 = np.random.randn(30,784)\n",
    "x_2 = np.random.randn(784,1)\n",
    "z_2 = np.dot(w_2, x_2)+b_2\n",
    "derv_2 = sigmoid_derivative(z_2)\n",
    "# print(z_2.shape, z_2)\n",
    "# print(derv_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "from_scratch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
